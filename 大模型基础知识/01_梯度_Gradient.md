# 梯度（Gradient）

## 简单理解

**梯度**就是"方向指南"，告诉模型参数应该往哪个方向调整、调整多少，才能让预测结果更准确。

---

## 类比解释

想象你蒙着眼睛站在山上，想要走到山谷最低点：

```
        你站在这里
            ↓
    ╱╲    ●
   ╱  ╲  ╱ ╲
  ╱    ╲╱   ╲
 ╱           ╲
╱      ★      ╲   ← 目标：山谷最低点（loss 最小）
```

- **梯度** = 脚下地面的坡度（告诉你哪边更陡）
- **梯度方向** = 最陡峭的上坡方向
- **下山方向** = 梯度的反方向（往低处走）
- **学习率** = 每一步走多远

---

## 数学定义

梯度是损失函数对每个参数的**偏导数**：

```
梯度 = ∂Loss / ∂参数

例如模型有 3 个参数 [w1, w2, w3]：
梯度 = [∂Loss/∂w1, ∂Loss/∂w2, ∂Loss/∂w3]
     = [0.5, -0.3, 0.8]  ← 每个参数应该调整的方向和幅度
```

---

## 训练过程中的梯度

```
┌─────────────────────────────────────────────────────────────┐
│  1. 前向传播                                                 │
│     输入数据 → 模型计算 → 预测结果 → 计算 Loss               │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  2. 反向传播（计算梯度）                                      │
│     从 Loss 出发，逐层计算每个参数的梯度                       │
│     这一步会占用大量显存（需要保存中间结果）                    │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  3. 参数更新                                                 │
│     新参数 = 旧参数 - 学习率 × 梯度                           │
│     例如：w1_new = w1_old - 0.0001 × 0.5                     │
└─────────────────────────────────────────────────────────────┘
```

---

## 代码示例

```python
import torch

# 创建一个需要计算梯度的参数
w = torch.tensor([2.0], requires_grad=True)

# 前向传播：计算 loss
loss = (w - 5) ** 2  # 目标是让 w 接近 5

# 反向传播：计算梯度
loss.backward()

print(f"参数值: {w.item()}")        # 2.0
print(f"梯度值: {w.grad.item()}")   # -6.0 (表示 w 应该增大)

# 参数更新
learning_rate = 0.1
w_new = w - learning_rate * w.grad
print(f"更新后: {w_new.item()}")    # 2.6 (朝着 5 的方向移动了)
```

---

## 与训练参数的关系

| 参数 | 与梯度的关系 |
|------|-------------|
| `gradient_accumulation_steps` | 累积多步梯度后再更新，模拟大 batch |
| `gradient_checkpointing` | 不保存中间结果，需要时重新计算梯度，省显存 |
| `learning_rate` | 梯度乘以学习率 = 实际更新量 |
| `deepspeed zero2/3` | 将梯度分片存储到多个 GPU |

---

## 显存占用

对于 8B 参数的模型：
- **梯度大小** = 参数量 × 精度 = 8B × 2 bytes (bfloat16) = **16GB**
- ZeRO-2/3 会将梯度分片：16GB ÷ 5 卡 = **3.2GB/卡**

---

## 一句话总结

> **梯度**告诉模型"你错了多少、往哪边改"，是深度学习训练的核心。

