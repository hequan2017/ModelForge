# 混合精度训练（Mixed Precision Training）

## 简单理解

**混合精度训练**就是"能省则省"——用低精度（16位）做计算加速，用高精度（32位）保存关键数据，两全其美。

---

## 类比解释

```
全精度训练（FP32）：
┌─────────────────────────────────────┐
│ 📝 所有计算都用双倍精度纸张           │
│ 💰 纸张贵（显存大）                   │
│ ⏱️ 写字慢（计算慢）                   │
│ ✅ 字迹清晰（精度高）                 │
└─────────────────────────────────────┘

混合精度训练：
┌─────────────────────────────────────┐
│ 📝 草稿用便签纸（FP16），重要的用好纸（FP32）│
│ 💰 省纸（省显存约 50%）               │
│ ⏱️ 写得快（计算快 2-3 倍）            │
│ ✅ 关键信息仍然清晰                   │
└─────────────────────────────────────┘
```

---

## 数据精度对比

```
FP32（32位浮点数）：
┌────────┬───────────────────────┬────────────────────────┐
│ 符号位 │      指数位（8位）      │       尾数位（23位）     │
└────────┴───────────────────────┴────────────────────────┘
- 范围：±3.4 × 10³⁸
- 精度：约 7 位有效数字
- 大小：4 字节

FP16（16位浮点数）：
┌────────┬──────────────┬──────────────┐
│ 符号位 │  指数位（5位） │  尾数位（10位）│
└────────┴──────────────┴──────────────┘
- 范围：±65504
- 精度：约 3 位有效数字
- 大小：2 字节

BF16（Brain Float 16）：
┌────────┬───────────────────────┬────────┐
│ 符号位 │      指数位（8位）      │尾数（7位）│
└────────┴───────────────────────┴────────┘
- 范围：与 FP32 相同（±3.4 × 10³⁸）
- 精度：约 2 位有效数字
- 大小：2 字节
```

---

## 为什么用 BF16？

```
FP16 的问题：
┌─────────────────────────────────────┐
│ 数值范围小，容易溢出                  │
│                                     │
│ 梯度值：0.00001 → 下溢变成 0          │
│ Loss值：100000 → 上溢变成 Inf         │
└─────────────────────────────────────┘

BF16 的优势：
┌─────────────────────────────────────┐
│ 数值范围与 FP32 相同，不容易溢出       │
│ 精度略低，但对训练影响不大            │
│                                     │
│ 大模型训练的首选精度！                │
└─────────────────────────────────────┘
```

---

## 混合精度工作原理

```
┌─────────────────────────────────────────────────────────┐
│                    混合精度训练流程                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  FP32 主权重（Master Weights）                           │
│       │                                                 │
│       │ 复制                                            │
│       ▼                                                 │
│  BF16 权重副本 ──→ 前向传播（BF16）──→ Loss（BF16）       │
│                                           │             │
│                                           │ 反向传播     │
│                                           ▼             │
│  FP32 主权重 ←── 更新 ←── BF16 梯度                      │
│       ↑                                                 │
│       └── 优化器状态（FP32）                             │
│                                                         │
└─────────────────────────────────────────────────────────┘

关键点：
1. 计算用 BF16（快）
2. 权重存储用 FP32（准）
3. 优化器状态用 FP32（稳）
```

---

## 显存节省

```
8B 参数模型对比：

全 FP32：
- 参数：8B × 4 bytes = 32 GB
- 梯度：8B × 4 bytes = 32 GB
- 优化器：8B × 8 bytes = 64 GB
- 总计：128 GB

混合精度（BF16）：
- 参数：8B × 2 bytes = 16 GB
- 梯度：8B × 2 bytes = 16 GB
- 优化器：8B × 8 bytes = 64 GB（仍用 FP32）
- 总计：96 GB

节省：128 - 96 = 32 GB（25%）

加上激活值的节省，实际可节省 40-50% 显存
```

---

## 训练速度提升

```
GPU 对不同精度的计算能力（以 A100 为例）：

FP32：19.5 TFLOPS
FP16：312 TFLOPS（Tensor Core）
BF16：312 TFLOPS（Tensor Core）

BF16 比 FP32 快约 16 倍！

实际训练中，由于内存带宽等限制，
通常能获得 2-3 倍的速度提升。
```

---

## 代码示例

### PyTorch 原生混合精度

```python
import torch
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
scaler = GradScaler()  # 梯度缩放器

for batch in dataloader:
    optimizer.zero_grad()
    
    # 自动混合精度上下文
    with autocast(dtype=torch.bfloat16):
        output = model(batch)
        loss = criterion(output, target)
    
    # 缩放 loss 并反向传播
    scaler.scale(loss).backward()
    
    # 更新参数
    scaler.step(optimizer)
    scaler.update()
```

### Transformers 中使用

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./output",
    bf16=True,              # 启用 BF16
    # 或者
    # fp16=True,            # 启用 FP16
)
```

### Swift 中使用

```bash
swift sft \
  --model Qwen/Qwen3-8B \
  --torch_dtype bfloat16 \  # 指定精度
  ...
```

---

## 精度选择建议

| GPU 类型 | 推荐精度 | 说明 |
|---------|---------|------|
| A100/H100 | BF16 | 原生支持，最佳选择 |
| RTX 3090/4090 | BF16 | 支持良好 |
| RTX 2080/V100 | FP16 | 不支持 BF16 |
| 老旧 GPU | FP32 | 不支持混合精度 |

```bash
# 检查 GPU 是否支持 BF16
python -c "import torch; print(torch.cuda.is_bf16_supported())"
```

---

## Loss Scaling（损失缩放）

FP16 训练时需要特殊处理：

```
问题：FP16 梯度太小会下溢变成 0

解决：Loss Scaling
┌─────────────────────────────────────────────┐
│ 1. 放大 Loss（如 ×1024）                     │
│ 2. 反向传播（梯度也被放大）                   │
│ 3. 更新前缩小梯度（÷1024）                   │
│                                             │
│ 这样小梯度就不会下溢了                        │
└─────────────────────────────────────────────┘

BF16 通常不需要 Loss Scaling，因为范围够大
```

---

## 常见问题

### Q: BF16 和 FP16 选哪个？

```
优先选 BF16：
- 数值稳定性更好
- 不需要 Loss Scaling
- 训练更稳定

只有 GPU 不支持 BF16 时才用 FP16
```

### Q: 混合精度会影响模型效果吗？

```
几乎不影响：
- 关键计算仍用高精度
- 大量实验证明效果相当
- 是目前大模型训练的标准做法
```

### Q: 为什么我的 Loss 变成 NaN？

```
可能原因：
1. FP16 数值溢出
2. Loss Scaling 设置不当

解决方法：
1. 换用 BF16
2. 减小学习率
3. 使用动态 Loss Scaling
```

---

## 与训练参数的关系

| 参数 | 说明 |
|------|------|
| `--torch_dtype bfloat16` | 使用 BF16 精度 |
| `--torch_dtype float16` | 使用 FP16 精度 |
| `--torch_dtype float32` | 使用 FP32 精度 |
| `--bf16 true` | Transformers 中启用 BF16 |
| `--fp16 true` | Transformers 中启用 FP16 |

---

## 一句话总结

> **混合精度训练**是"精打细算"的艺术——用 BF16 做计算省时省力，用 FP32 存关键数据保证精度，让大模型训练又快又稳。

