# 大模型基础知识

本目录包含大模型训练相关的基础知识文档，每个概念都配有详细的解释、类比、图解和代码示例。

## 📚 目录

| 序号 | 主题 | 文件 | 简介 |
|------|------|------|------|
| 01 | 梯度 | [01_梯度_Gradient.md](./01_梯度_Gradient.md) | 告诉模型"往哪改、改多少" |
| 02 | 损失函数 | [02_损失函数_Loss.md](./02_损失函数_Loss.md) | 衡量模型预测有多"差" |
| 03 | 学习率 | [03_学习率_Learning_Rate.md](./03_学习率_Learning_Rate.md) | 每次参数更新的"步长" |
| 04 | Batch Size | [04_Batch_Size_批次大小.md](./04_Batch_Size_批次大小.md) | 每次学习多少条数据 |
| 05 | Epoch | [05_Epoch_训练轮数.md](./05_Epoch_训练轮数.md) | 数据集被学习的"遍数" |
| 06 | 过拟合与欠拟合 | [06_过拟合与欠拟合.md](./06_过拟合与欠拟合.md) | 学不会 vs 死记硬背 |
| 07 | LoRA | [07_LoRA_低秩适配.md](./07_LoRA_低秩适配.md) | 高效微调，只训练"眼镜" |
| 08 | DeepSpeed ZeRO | [08_DeepSpeed_ZeRO.md](./08_DeepSpeed_ZeRO.md) | 分布式训练，分摊显存 |
| 09 | Transformer | [09_Transformer_架构.md](./09_Transformer_架构.md) | 大模型的"骨架" |
| 10 | Token | [10_Token_分词.md](./10_Token_分词.md) | 模型阅读的最小单位 |
| 11 | 优化器 | [11_优化器_Optimizer.md](./11_优化器_Optimizer.md) | 指导参数如何更新 |
| 12 | 混合精度 | [12_混合精度训练.md](./12_混合精度训练.md) | 省显存、加速训练 |

## 🎯 学习路线

### 入门级（必读）
1. **梯度** - 理解训练的核心机制
2. **损失函数** - 理解模型如何"知道"自己错了
3. **学习率** - 最重要的超参数之一
4. **Batch Size** - 理解显存和训练的关系

### 进阶级
5. **Epoch** - 控制训练时长
6. **过拟合与欠拟合** - 诊断训练问题
7. **优化器** - 理解 Adam/AdamW

### 高级（大模型专属）
8. **LoRA** - 高效微调必备
9. **DeepSpeed ZeRO** - 分布式训练必备
10. **混合精度** - 节省显存必备
11. **Transformer** - 理解模型架构
12. **Token** - 理解输入输出

## 🔧 快速参考

### 显存不足时的解决方案
1. 减小 `batch_size` → [04_Batch_Size](./04_Batch_Size_批次大小.md)
2. 启用梯度检查点 → [01_梯度](./01_梯度_Gradient.md)
3. 使用 LoRA 微调 → [07_LoRA](./07_LoRA_低秩适配.md)
4. 使用 ZeRO-3 → [08_DeepSpeed](./08_DeepSpeed_ZeRO.md)
5. 使用混合精度 → [12_混合精度](./12_混合精度训练.md)

### 训练不稳定时的解决方案
1. 减小学习率 → [03_学习率](./03_学习率_Learning_Rate.md)
2. 增大 batch size → [04_Batch_Size](./04_Batch_Size_批次大小.md)
3. 增大预热比例 → [03_学习率](./03_学习率_Learning_Rate.md)

### 模型效果不好时的解决方案
1. 检查是否过拟合/欠拟合 → [06_过拟合与欠拟合](./06_过拟合与欠拟合.md)
2. 调整学习率 → [03_学习率](./03_学习率_Learning_Rate.md)
3. 增加训练轮数 → [05_Epoch](./05_Epoch_训练轮数.md)
4. 增大 LoRA rank → [07_LoRA](./07_LoRA_低秩适配.md)

## 📖 推荐阅读顺序

```
新手：01 → 02 → 03 → 04 → 05 → 06
进阶：07 → 08 → 12
深入：09 → 10 → 11
```

## 🤝 贡献

欢迎补充更多基础知识文档！

