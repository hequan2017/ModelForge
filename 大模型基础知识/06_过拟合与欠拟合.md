# 过拟合与欠拟合（Overfitting & Underfitting）

## 简单理解

- **欠拟合**：模型太简单，没学会，考试全靠蒙
- **过拟合**：模型死记硬背，只会做原题，换个数就不会

---

## 类比解释

```
学生备考数学：

欠拟合（学渣）：
┌─────────────────────────────────────┐
│ 📖 课本都没看完                      │
│ 📝 考试：原题不会，新题也不会         │
│ 分数：30 分                          │
└─────────────────────────────────────┘

刚刚好（学霸）：
┌─────────────────────────────────────┐
│ 📖 理解了知识点和解题思路             │
│ 📝 考试：原题会做，新题也会举一反三    │
│ 分数：95 分                          │
└─────────────────────────────────────┘

过拟合（书呆子）：
┌─────────────────────────────────────┐
│ 📖 把习题册的答案全背下来了           │
│ 📝 考试：原题满分，但换个数字就懵了    │
│ 分数：原题 100 分，新题 40 分         │
└─────────────────────────────────────┘
```

---

## 图解

```
模型复杂度
    │
    │     欠拟合        刚好        过拟合
    │        │           │           │
    │        ▼           ▼           ▼
    │
    │     ─────       ～～～～      ∿∿∿∿∿∿
    │    （直线）    （平滑曲线）  （过度扭曲）
    │
    │    ● ●  ●      ●  ●  ●      ●  ●  ●
    │   ●    ●  ●   ●    ●  ●   ●    ●  ●
    │
    └────────────────────────────────────→ 数据点

欠拟合：模型太简单，无法捕捉数据规律
过拟合：模型太复杂，把噪声也学进去了
```

---

## Loss 曲线判断

### 欠拟合

```
Loss
  │
  │ ────────────── 训练 Loss（很高，下不去）
  │
  │ ────────────── 验证 Loss（也很高）
  │
  └──────────────────────→ Epoch

特点：训练 Loss 和验证 Loss 都很高
```

### 过拟合

```
Loss
  │
  │ ╲
  │  ╲____________ 训练 Loss（持续下降）
  │      ╱
  │     ╱
  │    ╱  验证 Loss（先降后升）
  │   ╱
  └──────────────────────→ Epoch
         ↑
      开始过拟合的点

特点：训练 Loss 低，验证 Loss 高（差距大）
```

### 刚刚好

```
Loss
  │
  │ ╲
  │  ╲
  │   ╲__________ 训练 Loss
  │    ╲________ 验证 Loss（两条线接近）
  │
  └──────────────────────→ Epoch

特点：训练 Loss 和验证 Loss 都低，且接近
```

---

## 原因分析

### 欠拟合的原因

| 原因 | 解决方法 |
|------|---------|
| 模型太简单 | 用更大的模型 |
| 训练时间太短 | 增加 Epoch |
| 学习率太小 | 增大学习率 |
| 特征不够 | 增加特征/数据预处理 |

### 过拟合的原因

| 原因 | 解决方法 |
|------|---------|
| 模型太复杂 | 用更小的模型或正则化 |
| 训练数据太少 | 增加数据或数据增强 |
| 训练时间太长 | 减少 Epoch，使用早停 |
| 学习率太大 | 减小学习率 |

---

## 解决过拟合的方法

### 1. 早停（Early Stopping）

```python
from transformers import TrainingArguments, EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="./output",
    evaluation_strategy="steps",
    eval_steps=50,
    load_best_model_at_end=True,  # 加载最佳模型
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    # 如果 3 次评估 Loss 都没下降，就停止训练
)
```

### 2. Dropout

```
正常前向传播：
○ → ○ → ○ → ○ → ○
所有神经元都参与

Dropout（训练时随机丢弃）：
○ → ✗ → ○ → ✗ → ○
部分神经元被随机关闭，防止过度依赖某些神经元
```

```python
# LoRA 微调中使用 dropout
--lora_dropout 0.05  # 5% 的 dropout
```

### 3. 正则化（Weight Decay）

```python
# 在损失函数中加入参数惩罚项
Loss_total = Loss_original + λ × ||参数||²

# 训练参数中设置
--weight_decay 0.01
```

### 4. 数据增强

```python
# 文本数据增强示例
原文本："今天天气很好"
增强后：
- "今天天气非常好"     # 同义词替换
- "天气很好今天"       # 随机打乱
- "今天天气很"         # 随机删除
- "今天的天气很好"     # 随机插入
```

### 5. 减少模型复杂度

```bash
# 使用更小的 LoRA rank
--lora_rank 32   # 而不是 64

# 或使用更小的模型
--model Qwen/Qwen3-4B  # 而不是 8B
```

---

## 解决欠拟合的方法

### 1. 增加训练时间

```bash
--num_train_epochs 5  # 从 3 增加到 5
```

### 2. 增大学习率

```bash
--learning_rate 5e-5  # 从 1e-5 增加到 5e-5
```

### 3. 使用更大的模型

```bash
--model Qwen/Qwen3-8B  # 从 4B 升级到 8B
```

### 4. 增加模型复杂度

```bash
# 使用更大的 LoRA rank
--lora_rank 128  # 从 64 增加到 128
```

---

## 实际案例

### 案例 1：过拟合

```
训练日志：
Epoch 1: train_loss=2.5, eval_loss=2.6
Epoch 2: train_loss=1.5, eval_loss=1.7
Epoch 3: train_loss=0.8, eval_loss=1.5  ← 差距开始变大
Epoch 4: train_loss=0.3, eval_loss=1.8  ← 验证 Loss 上升
Epoch 5: train_loss=0.1, eval_loss=2.2  ← 严重过拟合

解决：使用 Epoch 2 的模型，增加 dropout
```

### 案例 2：欠拟合

```
训练日志：
Epoch 1: train_loss=2.5, eval_loss=2.6
Epoch 2: train_loss=2.4, eval_loss=2.5
Epoch 3: train_loss=2.3, eval_loss=2.4  ← Loss 下降太慢

解决：增大学习率，增加 Epoch
```

---

## 与训练参数的关系

| 参数 | 过拟合时调整 | 欠拟合时调整 |
|------|------------|------------|
| `--num_train_epochs` | 减少 | 增加 |
| `--learning_rate` | 减小 | 增大 |
| `--lora_rank` | 减小 | 增大 |
| `--lora_dropout` | 增大 | 减小或去掉 |
| `--weight_decay` | 增大 | 减小 |
| 数据量 | 增加 | - |

---

## 一句话总结

> **欠拟合**是"没学会"，**过拟合**是"死记硬背"。好的模型要在两者之间找到平衡——既要学会规律，又要能举一反三。

